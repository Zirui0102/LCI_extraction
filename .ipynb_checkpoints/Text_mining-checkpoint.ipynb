{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf970eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (1.30.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (2.7.2)\n",
      "Requirement already satisfied: sniffio in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda\\envs\\lci-extraction\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f83358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.30.5\n"
     ]
    }
   ],
   "source": [
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0077a9d-510f-4e1d-81b0-b326abaf2716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5aeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from io import StringIO\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "     api_key=os.environ.get(\"OPENAI_API_KEY\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe5f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d4de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens1(text):\n",
    "    \"\"\"Counts the tokens in the given text.\"\"\"\n",
    "    return len(text.split())  # A simplistic approach, adjust based on your tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbeed37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:73: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:73: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\89751\\AppData\\Local\\Temp\\ipykernel_17116\\225823780.py:73: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  lines[i] = re.sub(pattern, '', lines[i]) if len(matches) > 500 and len(re.findall('\\d', matches)) < 8 and len(set(matches)) > 10 and matches.count(',') < 2 and len(matches) > 20 else lines[i]\n"
     ]
    }
   ],
   "source": [
    "def get_txt_from_pdf(pdf_files,filter_ref = True, combine=True):\n",
    "    \"\"\"Convert pdf files to dataframe\"\"\"\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "    # Iterate over the PDF\n",
    "    for pdf in pdf_files:\n",
    "        # Fetch the PDF content from the pdf\n",
    "        with open(pdf, 'rb') as pdf_content:\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "            # Iterate over all the pages in the PDF\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num] # Extract the text from the current page\n",
    "                page_text = page.extract_text()\n",
    "                words = page_text.split() # Split the page text into individual words\n",
    "                page_text_join = ' '.join(words) # Join the words back together with a single space between each word\n",
    "\n",
    "                if filter_ref: #filter the reference at the end\n",
    "                    page_text_join = remove_ref(page_text_join)\n",
    "\n",
    "                page_len = len(page_text_join)\n",
    "                div_len = page_len // 4 # Divide the page into 4 parts\n",
    "                page_parts = [page_text_join[i*div_len:(i+1)*div_len] for i in range(4)]\n",
    "            \n",
    "                min_tokens = 40\n",
    "                for i, page_part in enumerate(page_parts):\n",
    "                    if count_tokens(page_part) > min_tokens:\n",
    "                        # Append the data to the list\n",
    "                        data.append({\n",
    "                            'file name': pdf,\n",
    "                            'page number': page_num + 1,\n",
    "                            'page section': i+1,\n",
    "                            'content': page_part,\n",
    "                            'tokens': count_tokens(page_part)\n",
    "                        })\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "    if combine:\n",
    "        df = combine_section(df)\n",
    "    return df\n",
    "\n",
    "def remove_ref(pdf_text):\n",
    "    \"\"\"This function removes reference section from a given PDF text. It uses regular expressions to find the index of the words to be filtered out.\"\"\"\n",
    "    # Regular expression pattern for the words to be filtered out\n",
    "    pattern = r'(References|REFERENCES|Acknowledgment|ACKNOWLEDGMENT)'\n",
    "    match = re.search(pattern, pdf_text)\n",
    "\n",
    "    if match:\n",
    "        # If a match is found, remove everything after the match\n",
    "        start_index = match.start()\n",
    "        clean_text = pdf_text[:start_index].strip()\n",
    "    else:\n",
    "        # Define a list of regular expression patterns for references\n",
    "        reference_patterns = [\n",
    "            r'\\[\\w{1,3}\\].+?\\d{3,5}\\.', r'\\[\\w{1,3}\\].+?\\d{3,5};', r'\\(\\w{1,3}\\).+?\\d{3,5}\\.', r'\\[\\w{1,3}\\].+?\\d{3,5},',\n",
    "            r'\\(\\w{1,3}\\).+?\\d{3,5},', r'\\[\\w{1,3}\\].+?\\d{3,5}', r'\\w{1,3}\\).+?\\d{3,5}\\.', r'\\w{1,3}\\).+?\\d{3,5}',\n",
    "            r'\\(\\w{1,3}\\).+?\\d{3,5}', r'^[\\w\\d,\\.â€“ ;)-]+$',\n",
    "        ]\n",
    "\n",
    "        # Find and remove matches with the first eight patterns\n",
    "        for pattern in reference_patterns[:8]:\n",
    "            matches = re.findall(pattern, pdf_text, flags=re.S)\n",
    "            pdf_text = re.sub(pattern, '', pdf_text) if len(matches) > 500 and matches.count('.') < 2 and matches.count(',') < 2 and not matches[-1].isdigit() else pdf_text\n",
    "\n",
    "        # Split the text into lines\n",
    "        lines = pdf_text.split('\\n')\n",
    "\n",
    "        # Strip each line and remove matches with the last two patterns\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = line.strip()\n",
    "            for pattern in reference_patterns[7:]:\n",
    "                matches = re.findall(pattern, lines[i])\n",
    "                lines[i] = re.sub(pattern, '', lines[i]) if len(matches) > 500 and len(re.findall('\\d', matches)) < 8 and len(set(matches)) > 10 and matches.count(',') < 2 and len(matches) > 20 else lines[i]\n",
    "\n",
    "        # Join the lines back together, excluding any empty lines\n",
    "        clean_text = '\\n'.join([line for line in lines if line])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def combine_section(df):\n",
    "    \"\"\"Merge sections, page numbers, add up content, and tokens based on the pdf name.\"\"\"\n",
    "    aggregated_df = df.groupby('file name').agg({\n",
    "        'content': aggregate_content,\n",
    "        'tokens': aggregate_tokens\n",
    "    }).reset_index()\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "def aggregate_content(series):\n",
    "    \"\"\"Join all elements in the series with a space separator. \"\"\"\n",
    "    return ' '.join(series)\n",
    "\n",
    "def aggregate_tokens(series):\n",
    "    \"\"\"Sum all elements in the series.\"\"\"\n",
    "    return series.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10254439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    \"\"\"read pdf files from a specific directory\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            text += page.extract_text() if page.extract_text() else \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d21821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_SI(df):\n",
    "    \"\"\"Combine the SI document with the main part\"\"\"\n",
    "    df['normalized_name'] = df['file name'].str.replace('_SI', '', regex=False)\n",
    "    \n",
    "    grouped_df = df.groupby('normalized_name').agg({\n",
    "        'content': ' '.join,  # Concatenates content\n",
    "        'tokens': 'sum'       # Sums up the tokens\n",
    "    }).reset_index()\n",
    "\n",
    "    # Rename the columns\n",
    "    grouped_df.rename(columns={'normalized_name': 'file name'}, inplace=True)\n",
    "    \n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83ee4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_from_path(file_path):\n",
    "    \"\"\"Extract the title from the file name\"\"\"\n",
    "    file_name_with_extension = file_path.split('/')[-1]\n",
    "    # Remove the file extension\n",
    "    title = file_name_with_extension.replace('.pdf', '')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4623a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df, file_name):\n",
    "    \"\"\"Write a DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(file_name, index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9628d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(file_name):\n",
    "    \"\"\"Read a CSV file into a DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc292730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(input_string, max_tokens):\n",
    "    \"\"\"Splits a string into chunks based on a maximum token count.\"\"\"\n",
    "    split_strings = []\n",
    "    current_string = \"\"\n",
    "    tokens_so_far = 0\n",
    "\n",
    "    for word in input_string.split():\n",
    "        if tokens_so_far + count_tokens(word) > max_tokens:\n",
    "            # Add the current chunk to the split_strings and reset\n",
    "            split_strings.append(current_string.strip())\n",
    "            current_string = word  # Start new chunk with the current word\n",
    "            tokens_so_far = count_tokens1(word)  # Reset token count\n",
    "        else:\n",
    "            current_string += (\" \" if current_string else \"\") + word\n",
    "            tokens_so_far += count_tokens(word)\n",
    "\n",
    "    if current_string:  # Add the remaining part if any\n",
    "        split_strings.append(current_string.strip())\n",
    "\n",
    "    return split_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14b148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(df, max_tokens=10000):\n",
    "    \"\"\"Chunk the documents in the DataFrame based on token limits.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        document_title = row['title']  # Assuming 'title' column exists in the DataFrame.\n",
    "        content = row['content']\n",
    "        token_count = row.get('tokens', count_tokens(content))  # Default to count if not provided.\n",
    "        \n",
    "        if token_count > max_tokens:\n",
    "            parts = split_content(content, max_tokens)\n",
    "            for part_number, part in enumerate(parts, start=1):  # Start enumeration at 1.\n",
    "                chunk_content = part\n",
    "                token_count = count_tokens(part)\n",
    "                chunks.append({\n",
    "                    'title': f\"{document_title}_part_{part_number}\",\n",
    "                    'content': chunk_content,\n",
    "                    'tokens': token_count\n",
    "                })\n",
    "        else:\n",
    "            chunks.append({\n",
    "                'title': document_title,\n",
    "                'content': content,\n",
    "                'tokens': token_count\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b83a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(df):\n",
    "    \"\"\"Model 1 is to determine whether inventory data exists and their location in each article\"\"\"\n",
    "\n",
    "    response_msgs = []\n",
    "      \n",
    "    for _, row in df.iterrows():\n",
    "        title = row[df.columns[0]]\n",
    "        context = row['content']\n",
    "        \n",
    "        \n",
    "        system_msg = \"\"\"\n",
    "        Determine whether the provided context includes life cycle inventory data (or input-output data) related to LCA investigations. \n",
    "        \"\"\"\n",
    "\n",
    "        user_msg = \"\"\"\n",
    "        Context: \n",
    "        The main inputs and outputs of each subunit are listed in Table 4.\n",
    "        Answer: Yes, in Table 4.\n",
    "\n",
    "        Context: \n",
    "        In Table 3, Table 4 there is a summary of input and output data, respectively, for the whole methanol production process. These data refers to 1 kg of methanol produced (functional unit).\n",
    "        Answer: Yes, in Table 3 and Table 4.\n",
    "\n",
    "        Context: \n",
    "        For the researched route, the basic input and output energy consumption and specific pollution discharge data in the specific process are collected, such as raw material consumption, life cycle energy consumption, direct pollution discharge, etc. Taking the production of 1 ton of EG as the benchmark, Tables 2â€“4 list the energy consumption and pollutant emissions data at each stage of the life cycle of CtEG, OtEG, and BtEG routes.\n",
    "        Answer: Yes, in Tables 2-4.\n",
    "\n",
    "        Context: \n",
    "        Table S9 The inventory data for PV/CCU-CH3OH technical route.\n",
    "        Answer: Yes, in Table S9.\n",
    "\n",
    "        Context: \n",
    "        Information for biotechnologies for biomass to formic acid and methanol production are contained in Tables B1 â€“ B3. \n",
    "        Answer: Yes, in Tables B1-B3.\n",
    "\n",
    "        Context: \n",
    "        The inventories include wire for baling, diesel, and electricity at varying rates depending upon the waste stream entering the MRF (e.g. single stream, pre-sorted, mixed waste, dual stream) and the type of polymer being sorted (Table S2)\n",
    "        Answer: Yes, in Table S2.\n",
    "        \n",
    "        Context:\n",
    "        Table S1 details the input-output data of the four ethylene glycol production routes.\n",
    "        Answer: Yes, in Table S1.\n",
    "        \n",
    "        Context: \n",
    "        The plant-level mass and energy balances for manufacturing ethylene via the three pathways on the ethylene production scale of 1000 kt/yr are summarized in Table 1.\n",
    "        Answer: Yes, in Table 1.\n",
    "        \"\"\"\n",
    "\n",
    "        attempts = 3\n",
    "        while attempts > 0:\n",
    "            question = \"\"\"Determine whether the provided context includes life cycle inventory data (or input-output data) of LCA, \n",
    "        and answer with either \"Yes\" or \"No\" only.  If 'Yes,' specify the table number(s) where this data can be found (e.g., in Table 2). \n",
    "        If the data cannot be located, respond with 'cannot be located'. \n",
    "        \"\"\"\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\", \n",
    "                            \"content\": system_msg\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_msg + context + question\n",
    "                        }\n",
    "                    ],\n",
    "                    model='gpt-3.5-turbo')\n",
    "                \n",
    "                answers = response.choices[0].message.content\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                attempts -= 1\n",
    "                if attempts > 0:\n",
    "                    print(f\"Error: {str(e)}. Retrying in 60 seconds. {attempts} attempts remaining. (model 1)\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: Failed to process paper {title}. Skipping. (model 1)\")\n",
    "                    answers = \"No\"\n",
    "                    break\n",
    "\n",
    "        response_msgs.append(answers)\n",
    "    df['results'] = response_msgs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c4c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(df):\n",
    "    \"\"\"Model 2 is to summarize the biorefinery configurations\"\"\"\n",
    "\n",
    "    response_msgs = []\n",
    "      \n",
    "    for _, row in df.iterrows():\n",
    "        title = row[df.columns[0]]\n",
    "        context = row['content']\n",
    "    \n",
    "        attempts = 3\n",
    "        while attempts > 0:\n",
    "            question = \"\"\"Answer the questions as truthfully as possible using the provided context.\n",
    "            please summarize the main biorefinery configurations, such as temperatures, pressures, yield, and relative equipments, for each step\"\"\"\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": context + question\n",
    "                        }\n",
    "                    ],\n",
    "                    model='gpt-3.5-turbo')\n",
    "                \n",
    "                answers = response.choices[0].message.content\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                attempts -= 1\n",
    "                if attempts > 0:\n",
    "                    print(f\"Error: {str(e)}. Retrying in 60 seconds. {attempts} attempts remaining. (model 1)\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: Failed to process paper {title}. Skipping. (model 1)\")\n",
    "                    answers = \"No\"\n",
    "                    break\n",
    "\n",
    "        response_msgs.append(answers)\n",
    "    df['results1'] = response_msgs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9893cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/89751/OneDrive/desktop/GNN/Text_mining/Document/Techno-economic and environmental assessments for sustainable bio-methanol production as landfill gas valorization.pdf']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all pdf files from a specific directory\n",
    "directory = 'C:/Users/89751/OneDrive/desktop/GNN/Text_mining/Document/'\n",
    "files = os.listdir(directory)\n",
    "pdf_files = [os.path.join(directory, file) for file in files]  #\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b13534d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file name</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/89751/OneDrive/desktop/GNN/Text_minin...</td>\n",
       "      <td>Waste Management 150 (2022) 90â€“97 Available on...</td>\n",
       "      <td>10171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file name  \\\n",
       "0  C:/Users/89751/OneDrive/desktop/GNN/Text_minin...   \n",
       "\n",
       "                                             content  tokens  \n",
       "0  Waste Management 150 (2022) 90â€“97 Available on...   10171  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the content for each article\n",
    "df = get_txt_from_pdf(pdf_files)\n",
    "df\n",
    "#print(df['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c5f83e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file name</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/89751/OneDrive/desktop/GNN/Text_minin...</td>\n",
       "      <td>Waste Management 150 (2022) 90â€“97 Available on...</td>\n",
       "      <td>10171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file name  \\\n",
       "0  C:/Users/89751/OneDrive/desktop/GNN/Text_minin...   \n",
       "\n",
       "                                             content  tokens  \n",
       "0  Waste Management 150 (2022) 90â€“97 Available on...   10171  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine the main part with the supplementary document for each article\n",
    "df = combine_SI(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d93eeb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Techno-economic and environmental assessments ...</td>\n",
       "      <td>Waste Management 150 (2022) 90â€“97 Available on...</td>\n",
       "      <td>10171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Techno-economic and environmental assessments ...   \n",
       "\n",
       "                                             content  tokens  \n",
       "0  Waste Management 150 (2022) 90â€“97 Available on...   10171  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace file names with the article titles\n",
    "df['file name'] = df['file name'].apply(extract_title_from_path)\n",
    "df = df.rename(columns={'file name': 'title'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9a87b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Techno-economic and environmental assessments ...</td>\n",
       "      <td>Waste Management 150 (2022) 90â€“97 Available on...</td>\n",
       "      <td>9699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Techno-economic and environmental assessments ...</td>\n",
       "      <td>Farkhondehfal, M.A., Sastre, F., Makkee, M., S...</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Techno-economic and environmental assessments ...   \n",
       "1  Techno-economic and environmental assessments ...   \n",
       "\n",
       "                                             content  tokens  \n",
       "0  Waste Management 150 (2022) 90â€“97 Available on...    9699  \n",
       "1  Farkhondehfal, M.A., Sastre, F., Makkee, M., S...     460  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chunk the document if tokens > 10000\n",
    "df1 = chunk_documents(df)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0ac8787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Techno-economic and environmental assessments ...</td>\n",
       "      <td>Waste Management 150 (2022) 90â€“97 Available on...</td>\n",
       "      <td>9699</td>\n",
       "      <td>Yes, in Tables 2-4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Techno-economic and environmental assessments ...</td>\n",
       "      <td>Farkhondehfal, M.A., Sastre, F., Makkee, M., S...</td>\n",
       "      <td>460</td>\n",
       "      <td>Yes, in Tables S2, S9, B1-B3, 1, 3, and 4.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Techno-economic and environmental assessments ...   \n",
       "1  Techno-economic and environmental assessments ...   \n",
       "\n",
       "                                             content  tokens  \\\n",
       "0  Waste Management 150 (2022) 90â€“97 Available on...    9699   \n",
       "1  Farkhondehfal, M.A., Sastre, F., Makkee, M., S...     460   \n",
       "\n",
       "                                      results  \n",
       "0                         Yes, in Tables 2-4.  \n",
       "1  Yes, in Tables S2, S9, B1-B3, 1, 3, and 4.  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get results from the LLM about whether inventory data exists and locations\n",
    "result_df = model_1(df1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ab47097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, in the provided context, life cycle inventory data related to LCA investigations can be found.\n"
     ]
    }
   ],
   "source": [
    "print(df1['results'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "740a261e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>results</th>\n",
       "      <th>results1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LCA comparison analysis for two types of H2 ca...</td>\n",
       "      <td>RESEARCH ARTICLE LCA comparison analysis for t...</td>\n",
       "      <td>9378</td>\n",
       "      <td>Yes, in Table 1, Table 2.</td>\n",
       "      <td>The main biorefinery configurations include di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCA comparison analysis for two types of H2 ca...</td>\n",
       "      <td>Zhejiang University, Wiley Online Library on [...</td>\n",
       "      <td>6429</td>\n",
       "      <td>Yes, in Tables S1, S7, S8, S9, and S10.</td>\n",
       "      <td>The main biorefinery configurations for the pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  LCA comparison analysis for two types of H2 ca...   \n",
       "1  LCA comparison analysis for two types of H2 ca...   \n",
       "\n",
       "                                             content  tokens  \\\n",
       "0  RESEARCH ARTICLE LCA comparison analysis for t...    9378   \n",
       "1  Zhejiang University, Wiley Online Library on [...    6429   \n",
       "\n",
       "                                   results  \\\n",
       "0                Yes, in Table 1, Table 2.   \n",
       "1  Yes, in Tables S1, S7, S8, S9, and S10.   \n",
       "\n",
       "                                            results1  \n",
       "0  The main biorefinery configurations include di...  \n",
       "1  The main biorefinery configurations for the pr...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = model_2(df1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad33abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main biorefinery configurations include different temperatures, pressures, yields, and relative equipment for each step of the process. The steps involved in biorefinery processes typically include feedstock preparation, pretreatment, enzymatic hydrolysis, fermentation, and product recovery.\n",
      "\n",
      "1. Feedstock Preparation: The feedstock, such as biomass, is prepared for further processing by grinding, milling, or chopping to reduce particle size and increase surface area for better extraction and conversion efficiency.\n",
      "\n",
      "2. Pretreatment: In the pretreatment step, the feedstock is treated with heat, chemicals, or enzymes to break down complex structures like lignin and hemicellulose, making the cellulose more accessible for enzymatic hydrolysis. Pretreatment conditions may include temperatures ranging from 120-200Â°C and pressures of 5-30 bar.\n",
      "\n",
      "3. Enzymatic Hydrolysis: Enzymatic hydrolysis involves the use of enzymes to break down cellulose into fermentable sugars. The process operates at temperatures around 50-60Â°C and pressures of atmospheric conditions. The yield of fermentable sugars depends on the efficiency of the enzymes used and the effectiveness of the pretreatment step.\n",
      "\n",
      "4. Fermentation: Fermentation converts the fermentable sugars obtained from enzymatic hydrolysis into biofuels or other valuable products using microorganisms. The temperature for fermentation typically ranges from 25-35Â°C, and the pressure is usually atmospheric. The yield of biofuels depends on the type of microorganism used and the fermentation conditions.\n",
      "\n",
      "5. Product Recovery: In the product recovery step, the biofuels or other products are separated from the fermentation broth using techniques such as distillation, centrifugation, or filtration. The temperature, pressure, and equipment used for product recovery vary depending on the specific product being recovered.\n",
      "\n",
      "Overall, the biorefinery configurations for each step of the process are designed to optimize the conversion of biomass into valuable products while minimizing energy consumption and environmental impact.\n"
     ]
    }
   ],
   "source": [
    "print(df1['results1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37965ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export title and results as csv. file\n",
    "df_selected = result_df[['title', 'results', 'results1']]\n",
    "file_name = 'C:/Users/89751/OneDrive/desktop/test.csv'\n",
    "df_to_csv(df_selected, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c081e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
